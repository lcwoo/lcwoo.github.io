---
title: "OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving"
date: "2025-05-21"
summary: "paper review"
---

> 2024년 발표된 OpenEMMA 논문  
> [[paper]](https://arxiv.org/abs/2410.23262), [[GitHub]](https://github.com/taco-group/OpenEMMA)  
> Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu  
> The 3rd WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD)  
> 2024년 12월19일  

## 목차 / Table of Contents

1. 서론 (Introduction)
2. 방법론 (Method)
3. 모델 구조 (Architecture Overview)
4. 실험 및 분석 (Experiments & Analysis)
5. 한계(Limitations)
6. 결론 및 의의  

---  
 
<Image
  src="/images/paper-review/OpenEMMA/OpenEMMA_framework.png"
  alt="clip overview"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>


## 1. 서론 (Introduction)

자율주행 기술은 눈부신 속도로 발전하고 있지만, 실제 도로에서는 예측할 수 없는 교통 상황과 다양한 환경 조건 때문에 여전히 해결해야 할 과제가 많습니다. 
기존 자율주행 시스템은 Perception, Mapping, Prediction, Planning 등 기능을 각각 독립된 모듈로 구성하는 모듈형 프레임워크를 사용해 왔습니다.
모듈 단위로 디버깅하거나 최적화하기에는 유리하지만 모듈 간 통신 오류와 새로운 상황에 유연하게 대응하지 못하는 구조로 인해 확장성 부족이라는 한계가 있었습니다. 
최근에는 센서 데이터를 기반으로 직접 운전 행동을 예측하는 end-to-end 자율주행 시스템이 주목받고 있습니다. 
Multimodal Large Language Models(MLLMs)는 방대한 지식을 바탕으로 Chain-of-Thought 추론 능력을 갖추고 있어 
복잡한 도로 상황을 이해하고 추론하는 데 새로운 가능성을 제시합니다.
그 대표적인 예가 Google Gemini 기반의 EMMA 모델이지만 시스템이 비공개가 되어있어 일반 연구자가들이 사용하기 어려웠습니다.  
<HighlightText lightColor="red.300" darkColor="#ffb163">그래서 OpenEMMA에서 공개된 MLLM과 오픈소스 기반 도구들을 활용해 EMMA의 기능을 재현한 OpenEMMA를 제안합니다. </HighlightText> 
OpenEMMA는 전방 카메라 이미지와 주행 이력을 입력으로 받아 Visual Question Answering(VQA) 문제로 변환하여 경로 계획과 객체 인식을 수행합니다.

---  

### <HighlightText fontSize="2xl">핵심 contributions</HighlightText>

<HighlightText lightColor="blue.400" darkColor="yellow.400" > 
1. 사전 학습된 MLLM과 기존 오픈소스 모듈을 결합하여 
trajectory planning과 perception을 통합적으로 수행할 수 있는 end-to-end 프레임워크 OpenEMMA를 제안합니다.  
</HighlightText> 


<HighlightText lightColor="blue.400" darkColor="yellow.400" > 
<br />
2. 다양한 MLLM 백본을 적용해 nuScenes 데이터셋에서 OpenEMMA의 성능을 평가했고 기존의 zero-shot 방식보다 낮은 오차와 실패율을 달성함을 입증했습니다.  
</HighlightText> 


<HighlightText lightColor="blue.400" darkColor="yellow.400"> 
<br />
3. 연구를 이용하고 확장할 수 있게 논문에서 사용된 전체 코드, 데이터셋, 모델 가중치를 
GitHub(https://github.com/taco-group/OpenEMMA)에 공개하였습니다.  
</HighlightText> 

---

## 2. 방법론 (Method)

<Image
  src="/images/paper-review/OpenEMMA/openemma_demo.gif"
  alt="clip overview"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

>Illustration of OpenEMMA’s prediction.

OpenEMMA는 전방 카메라 이미지와 과거 5초간의 속도, 곡률를 입력으로 받아 
Chain-of-Thought 기반 reasoning과 예측을 통해 trajectory를 생성하는 end-to-end 자율주행 프레임워크입니다. 
객체 탐지 정확도를 보완하기 위해 YOLO3D을 통합하여 3D 객체 인식도 함께 수행합니다.  
MLLM은 자연어 기반의 추론을 통해 사람처럼 주행 상황을 이해하고 차량의 의도와 주행 환경을 고려하여 미래의 속도 및 회전 곡률을 생성합니다. 
정밀한 객체 인식이 필요한 경우에는 YOLO3D를 외부 모듈로 결합하여 3D 객체 감지를 수행하며 이 결과는 최종 시각화에만 사용됩니다.  

---

#### 2.1 Chain-of-Thought 기반 경로 계획 (Trajectory Planning)

<HighlightText lightColor="red.300" darkColor="#ffb163"> Stage 1: Reasoning </HighlightText>
<br />
- Intent Command: 좌/우회전, 직진 여부 및 속도 조절
- Scene Description: 교통 상황(신호등, 보행자, 차량 등)
- Major Objects: 주의가 필요한 객체의 위치 및 행동

MLLM은 입력 장면에 대해 세 가지 요소를 자연어로 추론합니다. 

```python
def SceneDescription(obs_images, processor=None, model=None, tokenizer=None, args=None):
    prompt = f"""You are a autonomous driving labeller. 
    You have access to these front-view camera images of a car taken at a 0.5 second interval over the past 5 seconds. 
    Imagine you are driving the car. 
    Describe the driving scene according to traffic lights, movements of other cars or pedestrians and lane markings."""

    if "llava" in args.model_path:
        prompt = f"""You are an autonomous driving labeller. 
        You have access to these front-view camera images of a car taken at a 0.5 second interval over the past 5 seconds. 
        Imagine you are driving the car. 
        Provide a concise description of the driving scene according to traffic lights, movements of other cars or pedestrians and lane markings."""

    result = vlm_inference(text=prompt, images=obs_images, processor=processor, model=model, tokenizer=tokenizer, args=args)
    return result
```

<HighlightText lightColor="red.300" darkColor="#ffb163"> Stage 2: Predicting </HighlightText>
<br />
- 미래 $T$초간의 속도 $S = \{s_t\}_{t=0}^{T}$
- 곡률 $ K = \{k_t\}_{t=0}^{T} $
→ 이를 통해 최종 경로 ${\{x_t, y_t\}_{t=0}^{T}}$ 생성

예측된 수치 리스트는 정규식으로 파싱하여 수치로 변환한 후 아래 적분 모델을 통해 $(x_t, y_t)$ 형태의 trajectory로 환산됩니다.

```python
coordinates = re.findall(r"\[([-+]?\d*\.?\d+),\s*([-+]?\d*\.?\d+)\]", prediction)
speed_curvature_pred = [[float(v), float(k)] for v, k in coordinates]
```


<HighlightText lightColor="red.300" darkColor="#ffb163"> Stage 2: Predicting </HighlightText>
<br />
예측된 속도 및 곡률은 다음 수식을 기반으로 통합되어 ego-trajectory를 생성합니다.

$$
\theta_t = \theta_{t-1} + \int_{t-1}^{t} k(\tau) \cdot s(\tau) \, d\tau
$$

$$
v_x(t) = s_t \cdot \cos(\theta_t), \quad v_y(t) = s_t \cdot \sin(\theta_t)
$$

$$
x_t = x_{t-1} + \int_{t-1}^{t} v_x(\tau) \, d\tau, \quad
y_t = y_{t-1} + \int_{t-1}^{t} v_y(\tau) \, d\tau
$$

실제 구현에서는 위 식을 수치적으로 근사하여 다음처럼 계산합니다:
$$
\theta_t \approx \theta_0 + \sum_{i=1}^{t} k_i \cdot s_i \cdot \Delta t
$$
$$
x_t \approx x_0 + \sum_{i=1}^{t} v_x(i) \cdot \Delta t
$$
$$
y_t \approx y_0 + \sum_{i=1}^{t} v_y(i) \cdot \Delta t
$$

```python
# cumulative_trapezoid를 활용한 수치 적분
theta = cumulative_trapezoid(curvatures * velocities, t, initial=0)
v_x = velocities * np.cos(theta)
v_y = velocities * np.sin(theta)
x = cumulative_trapezoid(v_x, t, initial=0)
y = cumulative_trapezoid(v_y, t, initial=0)
```
위 계산은 코드상 IntegrateCurvatureForPoints() 함수에 구현되어 있습니다.  

```python
def IntegrateCurvatureForPoints(curvatures, velocities_norm, initial_position, initial_heading, time_span):
    t = np.linspace(0, time_span, time_span)
    theta = cumulative_trapezoid(curvatures * velocities_norm, t, initial=0) + initial_heading
    v_x = velocities_norm * np.cos(theta)
    v_y = velocities_norm * np.sin(theta)
    x = cumulative_trapezoid(v_x, t, initial=0) + initial_position[0]
    y = cumulative_trapezoid(v_y, t, initial=0) + initial_position[1]
    return np.stack((x, y), axis=1)
```

> OpenEMMA는 사전 학습된 MLLM을 활용해 별도의 fine-tuning 없이 zero-shot 방식으로 경로를 예측합니다.

---  

#### 2.2 시각 전문가 기반 객체 탐지 (Visual Specialist for Object Detection)

YOLO3D는 정확한 공간적 객체 탐지를 위해 별도로 통합된 lightweight 3D detector입니다. 
OpenEMMA는 단일 정면 이미지를 기반으로 YOLO3D를 적용하여 2D-3D bounding box를 추론하며 결과는 trajectory와 함께 시각화 용도로만 활용됩니다. 

$$
\text{3D Bounding Box} = (t_x, t_y, t_z, d_x, d_y, d_z, \theta)
$$

```python
img = yolo3d_nuScenes(image, calib=obs_camera_params[-1])[0]
```

YOLO3D는 trajectory 예측에는 관여하지 않으며 결과 이미지를 시각적으로 보완하는 용도로만 사용됩니다. 

---


## 3. 실험 및 분석 (Experiments & Analysis)

OpenEMMA의 경로 예측 성능을 검증하기 위해 nuScenes validation set의 150개 주행 시나리오를 사용하였습니다. 
각 시나리오에 대해 0.5초 간격, 총 10개 지점으로 구성된 5초 간의 미래 trajectory를 예측하도록 설정하였습니다.  

OpenEMMA는 다양한 사전 학습 MLLM을 백본으로 사용하여 예측을 수행하였으며 
추론(Reasoning) 단계를 생략한 zero-shot baseline과 비교 실험을 진행하였습니다.

백본 모델:

- GPT-4o

- LLaVA-1.6-Mistral-7B

- LLaMA-3.2-11B-Vision-Instruct

- Qwen2-VL-7B-Instruct

Zero-shot baseline: reasoning 없이 카메라 이미지와 과거 주행 이력만을 기반으로 MLLM에게 직접 예측을 요청한 방식

<Image
  src="/images/paper-review/OpenEMMA/e2e_trajectory_experiments.png"
  alt="End-to-end trajectory planning experiments on nuScenes"
  width={800}
  height={450}
  style={{ margin: '0 auto', display: 'block' }}
/>

- L2 Distance (ADE): 예측된 trajectory와 ground-truth trajectory 사이의 평균 L2 거리 1초, 2초, 3초 시점에서 측정
- Failure Rate: 1초 내 L2 오류가 10m를 초과한 비율

- LLaVA-1.6-Mistral-7B를 백본으로 사용할 때 OpenEMMA는 zero-shot baseline 대비 L2 오류와 failure rate 모두에서 크게 향상된 성능을 보였습니다.

- Qwen2-VL-7B-Instruct의 경우 일부 어려운 장면에서도 예측은 수행되었지만 trajectory의 정밀도가 낮아 L2 오류는 증가한 반면 failure rate는 크게 감소하였습니다.

- GPT-4o는 제한된 케이스에서만 평가되었으며 높은 reasoning 성능을 보여주었으나 실험 범위가 제한적이었습니다.

---

## 5. 한계 (Limitations)

OpenEMMA는 사전 학습된 MLLM과 간단한 Chain-of-Thought 추론만으로도 자율주행의 핵심 기능인 trajectory 예측과 객체 인식을 일정 수준까지 수행할 수 있음을 입증하였습니다. 
그러나 현재 프레임워크에는 몇 가지 근본적인 한계가 존재합니다.  
<HighlightText lightColor="red.300" darkColor="#ffb163"> reasoning 과정에서 사용된 Chain-of-Thought는 매우 기초적인 수준에 머물고 있습니다. </HighlightText>
단순한 시나리오 설명 및 의도 추론에 기반해 속도와 곡률을 생성하는 방식은 복잡한 도로 상황이나 장기적인 의사결정을 완벽하게 반영하기 어렵습니다. 
향후 연구에서는 CoT-SC(Self-Consistency)나 ToT(Tree-of-Thought)와 같은 더 정교한 추론 전략을 도입함으로써 추론 과정의 일관성과 신뢰도를 높일 수 있을 것입니다.  

<HighlightText lightColor="red.300" darkColor="#ffb163"> 현시점의 MLLM은 시각적 공간 이해와 객체 정밀 인식 면에서 한계를 보입니다. </HighlightText>
이러한 이유로 OpenEMMA는 YOLO3D와 같은 외부의 시각 전문가를 추가적으로 통합하여 객체 인식 기능을 보완하였습니다. 
이는 MLLM만으로 자율주행의 모든 인식 기능을 수행하기에는 아직 기술적인 격차가 존재함을 보여주는 사례입니다.  

이러한 제약을 극복하기 위해서는 MLLM 자체의 시각 grounding 능력과 3D 객체 이해 성능을 강화하는 연구가 병행되어야 합니다.  
<HighlightText lightColor="red.300" darkColor="#ffb163"> 장기적으로는 외부 모듈에 의존하지 않고 MLLM 하나로 인식과 추론 계획까지 모두 통합할 수 있는 truly end-to-end 구조로 발전시키는 것이 목표입니다. </HighlightText>

---

## 6. 결론 및 의의 (Conclusion & Significance)

본 논문에서는 MLLM을 기반으로 한 오픈소스 자율주행 프레임워크인 OpenEMMA를 제안하였습니다. 
OpenEMMA는 전방 카메라 이미지와 과거 5초간의 속도 및 곡률 정보를 입력으로 받아 Chain-of-Thought 기반 reasoning을 통해 
미래 속도와 곡률을 예측하고 이를 통합하여 차량의 미래 경로를 생성합니다. 
또한 MLLM만으로는 부족할 수 있는 시각 인식 성능을 보완하기 위해 YOLO3D를 외부 모듈로 통합하여 3D 객체 인식 기능도 함께 수행하였습니다. 
이러한 구조는 MLLM의 자연어 기반 reasoning 능력을 적극 활용함으로써 추가적인 학습 없이도 다양한 주행 시나리오에 대응할 수 있는 유연한 계획 능력을 제공합니다.  
OpenEMMA는 zero-shot 방식으로 단순히 예측만 수행하는 기존 baseline에 비해 L2 trajectory 오류와 failure rate 측면에서 모두 우수한 성능을 보였습니다. 
LLaVA-1.6-Mistral-7B와 같은 모델을 사용할 경우 reasoning 기반 예측이 성능 향상으로 이어짐을 확인하였습니다. 
OpenEMMA는 학습된 MLLM을 기반으로 추가 학습 없이 prompt 기반의 reasoning만으로 
driving trajectory를 생성하는 zero-shot driving policy 구현 가능성을 보여주며 
자율주행 분야에서 MLLM의 활용 가능성을 제시한 의미 있는 초기 사례입니다.
